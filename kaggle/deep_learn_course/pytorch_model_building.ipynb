{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module & Parameters\n",
    "\n",
    "Module and Parameter are how we define a neural network model in Pytorch. They are given by library `torch.nn.Module` and `torch.nn.Parameter`. \n",
    "\n",
    "Define a tiny model with 2 linear layers,\n",
    "- 1st Linear Layer : Input size = 200, Output size = 100, activation function = ReLU\n",
    "- 2nd Linear Layer : Input size = 200, Output size = 10, activation function = softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tiny model\n",
    "class TinyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(in_features=100, out_features=200)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(in_features=200, out_features=10)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyModel(\n",
      "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n",
      "Linear(in_features=200, out_features=10, bias=True)\n",
      "\n",
      "\n",
      "Layer params:\n",
      "Parameter containing:\n",
      "tensor([[ 0.0671, -0.0373,  0.0059,  ...,  0.0362,  0.0368, -0.0116],\n",
      "        [-0.0673, -0.0417,  0.0070,  ..., -0.0311,  0.0361,  0.0162],\n",
      "        [ 0.0387,  0.0154, -0.0532,  ..., -0.0027, -0.0343,  0.0588],\n",
      "        ...,\n",
      "        [ 0.0032, -0.0188,  0.0638,  ..., -0.0602,  0.0433,  0.0691],\n",
      "        [-0.0705,  0.0332, -0.0491,  ...,  0.0665, -0.0046,  0.0087],\n",
      "        [-0.0506, -0.0153,  0.0589,  ..., -0.0664, -0.0575, -0.0069]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0376,  0.0694, -0.0425,  0.0447,  0.0314, -0.0348,  0.0143,  0.0132,\n",
      "        -0.0621,  0.0297], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "tinymodel = TinyModel()\n",
    "\n",
    "print(tinymodel)\n",
    "print(tinymodel.linear2)\n",
    "print('\\n\\nLayer params:')\n",
    "for param in tinymodel.linear2.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Layer Types\n",
    "\n",
    "- Linear Layers\n",
    "- Convolution Layers\n",
    "- Recurrent Layers\n",
    "- Data Manipulation Layers: Max Pooling, Normalization, Dropout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "\n",
    "The main usage of Activation function is to introduce the nonlinearity in the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "The Loss function measures the disparity btween the target's true value and the value model predicts.\n",
    "\n",
    "**Optimizer** tells the neural network how to solve the problem. It is the algorithm that adjusts the weights to minimize the loss. Virtually all optimization algorithms is based on **Stochastic Gradient Decent**. \n",
    "- **Adam** is an SGD algorithm that has an adaptive learning rate that makes it general purpose optimizer suitable for most problems without any parameter tuning (for learning rate)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
