{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a068c9a",
   "metadata": {},
   "source": [
    "# Next Character Prediction\n",
    "\n",
    "This notebook contains the implementation of the next character prediction multi layer preceptron (MLP) model from Anrej Karparty's makemore series. \n",
    "\n",
    "Ref:\n",
    "1. [Jupyter Notebook](https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part2_mlp.ipynb)\n",
    "2. [makemore](https://github.com/karpathy/makemore)\n",
    "3. [Benjio's Paper: A Neural Probabilistic Language model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e909ffb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52ca485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as f\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5391617",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6764612f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia'],\n",
       " 32033)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8], len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564a57e4",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "286f1f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build vocabulary of character mapping to and from integers\n",
    "chars = sorted(set((\"\".join(words))))\n",
    "stoi = {c:i+1 for i,c in enumerate(chars)}\n",
    "stoi['.'] = 0 #add character . to top of map\n",
    "itos = {i:c for c,i in stoi.items()}\n",
    "\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5995a0",
   "metadata": {},
   "source": [
    "## Build Dataset\n",
    "\n",
    "Inspired by Benjio's paper the model uses three character tokens to predict the next token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8626c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataset\n",
    "\n",
    "def build_dataset(words):\n",
    "    block_size = 3 # contxt length\n",
    "    X, Y = [], []\n",
    "    for w in words:#[:5]:\n",
    "        # print(w)\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            # print(''.join([itos[i] for i in context]),\"--->\",itos[ix])\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X,Y\n",
    "\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "# train, val/dev/ test split\n",
    "# 80% , 10%, 10%\n",
    "n1 = int(len(words)*0.8)\n",
    "n2 = int(len(words)*0.9)\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6624d6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([182625, 3]), torch.Size([182625]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape, Ytr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db351d3d",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The model is a multi layer perceptron neural network, which is inspired from Benjio's paper. \n",
    "\n",
    "It consists of 3 layers\n",
    "1. Embedding layer\n",
    "2. Hidden Layer\n",
    "3. Output Layer\n",
    "\n",
    "![MLP Architecture](mlp_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26744da9",
   "metadata": {},
   "source": [
    "Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3f1995c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The embedding layer Look up table\n",
    "emb_dim = 2 # dimension of each character embedding\n",
    "# Lookup table\n",
    "C = torch.randn((27,emb_dim)) # 26 alphabets + 1 ('.') character \n",
    "# input embedding\n",
    "emb = C[X]\n",
    "\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768c0c3e",
   "metadata": {},
   "source": [
    "Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "590beb8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1 = torch.randn((6, 100)) # 3*2 (3 characters of 2 dimension each)\n",
    "b1 = torch.randn(100)\n",
    "h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n",
    "\n",
    "# b1 broadcasting check\n",
    "# emb@W1 : 32  100\n",
    "# b1     :  1  100\n",
    "\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44ac6b7",
   "metadata": {},
   "source": [
    "Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5d2036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100,27))\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07a2bfd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.8391)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = h @ W2 + b2\n",
    "counts = logits.exp()\n",
    "probs = counts/counts.sum(dim=1, keepdim=True)\n",
    "loss = -probs[torch.arange(32),Y].log().mean() # negative log likelihood of probabilities\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa463a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.8391)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use cross_entrophy function to do the above steps from logits efficiently\n",
    "# cross entropy not only optimizes the computation memory but also makes it numerically stable.\n",
    "# when logits is returning big positive number counts becomes extremly large number (inf)\n",
    "# inorder to deal with this, this function subtracts all logits with max(logits)  \n",
    "f.cross_entropy(logits, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f5810",
   "metadata": {},
   "source": [
    "### Full Network model: Emb_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f5f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # forward pass\n",
    "# def forward(X,C,W1,b1,W2,b2):\n",
    "#     # input embedding\n",
    "#     emb = C[X]\n",
    "#     h = torch.tanh(emb.view(-1,6) @ W1 + b1)\n",
    "#     logits = h @ W2 + b2\n",
    "#     return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "1995b4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network initializations\n",
    "emb_dim = 2 # dimension of each character embedding\n",
    "context_len = 3 # number or input characters / context lenghts\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27,emb_dim), generator=g) # 26 alphabets + 1 ('.') character \n",
    "W1 = torch.randn((emb_dim*context_len, 300), generator=g) # 3*2 (3 characters of 2 dimension each)\n",
    "b1 = torch.randn(300, generator=g)\n",
    "W2 = torch.randn((300,27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "ed4094d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10281"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # total number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "bd54bafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "af509d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning Learning rate lr\n",
    "# lower bound : 1e-3, upper bound: 1\n",
    "lre = torch.linspace(-3, 0, 1000) # learning rate exponents\n",
    "lrs = 10**lre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "fa3a4711",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30_000     # number of training iterations\n",
    "BATCH_SIZE = 32 # mini-batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "1fa93b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrei = []\n",
    "lossi =[]\n",
    "stepi =[]\n",
    "\n",
    "# training loop\n",
    "for i in range(EPOCHS):\n",
    "    # mini-batch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (BATCH_SIZE,))\n",
    "    \n",
    "    # Forward pass\n",
    "    emb = C[Xtr[ix]]  # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1,emb_dim*context_len) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2    # (32, 27)\n",
    "    loss = f.cross_entropy(logits,Ytr[ix])\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters: \n",
    "        p.grad = None # set zero-gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = 0.001 #lrs[i]\n",
    "    for p in parameters:\n",
    "        p.data += -lr*p.grad\n",
    "\n",
    "    # # track-stats\n",
    "    # lrei.append(lre[i])\n",
    "    stepi.append(i)\n",
    "    lossi.append(loss.item())\n",
    "\n",
    "\n",
    "\n",
    "# print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "d99a2a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.188110589981079"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass\n",
    "emb = C[Xtr]  # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1,emb_dim*context_len) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2    # (32, 27)\n",
    "loss = f.cross_entropy(logits,Ytr)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "ad3b9444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning rates\n",
    "# plt.plot(lrei, lossi)\n",
    "# plt.plot(stepi, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5697899e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.203530788421631"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass\n",
    "emb = C[Xdev]  # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1,emb_dim*context_len) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2    # (32, 27)\n",
    "loss = f.cross_entropy(logits,Ydev)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "2a21316e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from turtle import color\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(C[:,0].data, C[:,1].data, s=200)\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i,0].item(), C[i,1].item(), itos[i], ha=\"center\", va='center', color='white')\n",
    "plt.grid('minor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7600b512",
   "metadata": {},
   "source": [
    "### Full Network Model: Emb_dim=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "e2004275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network initializations\n",
    "emb_dim = 10 # dimension of each character embedding\n",
    "context_len = 3 # number or input characters / context lenghts\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27,emb_dim), generator=g) # 26 alphabets + 1 ('.') character \n",
    "W1 = torch.randn((emb_dim*context_len, 300), generator=g) # 3*2 (3 characters of 2 dimension each)\n",
    "b1 = torch.randn(300, generator=g)\n",
    "W2 = torch.randn((300,27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "a3a477bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17697"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # total number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "64a76beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "0582bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tuning Learning rate lr\n",
    "# # lower bound : 1e-3, upper bound: 1\n",
    "# lre = torch.linspace(-3, 0, 1000) # learning rate exponents\n",
    "# lrs = 10**lre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "f37ab179",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200_000     # number of training iterations\n",
    "BATCH_SIZE = 32 # mini-batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "af9b0100",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrei = []\n",
    "lossi =[]\n",
    "stepi =[]\n",
    "\n",
    "# training loop\n",
    "for i in range(EPOCHS):\n",
    "    # mini-batch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (BATCH_SIZE,))\n",
    "    \n",
    "    # Forward pass\n",
    "    emb = C[Xtr[ix]]  # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1,emb_dim*context_len) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2    # (32, 27)\n",
    "    loss = f.cross_entropy(logits,Ytr[ix])\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters: \n",
    "        p.grad = None # set zero-gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100_000 else 0.01 if i < 150_000 else 0.001#lrs[i]\n",
    "    for p in parameters:\n",
    "        p.data += -lr*p.grad\n",
    "\n",
    "    # # track-stats\n",
    "    # lrei.append(lre[i])\n",
    "    stepi.append(i)\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "\n",
    "\n",
    "# print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "f0c94520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(stepi,lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "37ffe187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1012520790100098"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass\n",
    "emb = C[Xtr]  # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1,emb_dim*context_len) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2    # (32, 27)\n",
    "loss = f.cross_entropy(logits,Ytr)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "17d92fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1674981117248535"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass\n",
    "emb = C[Xdev]  # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1,emb_dim*context_len) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2    # (32, 27)\n",
    "loss = f.cross_entropy(logits,Ydev)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89be03f",
   "metadata": {},
   "source": [
    "## Sample from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "b0c6da70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carmah.\n",
      "amelle.\n",
      "khy.\n",
      "mili.\n",
      "tatyah.\n",
      "cassie.\n",
      "rahnen.\n",
      "delynn.\n",
      "jareei.\n",
      "ner.\n",
      "kia.\n",
      "chaiivia.\n",
      "leigh.\n",
      "ham.\n",
      "join.\n",
      "quinton.\n",
      "lilea.\n",
      "jadii.\n",
      "wazell.\n",
      "dearyn.\n"
     ]
    }
   ],
   "source": [
    "# sample from the network model\n",
    "with torch.no_grad():\n",
    "    g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "    for _ in range(20):\n",
    "        out = []\n",
    "        context = [0] * context_len # initialize with all ...\n",
    "        while True:\n",
    "            # forward pass\n",
    "            emb = C[torch.tensor([context])] # 1, 3, 10\n",
    "            h = torch.tanh(emb.view(-1, emb_dim*context_len)@W1 + b1)\n",
    "            logits = h@W2 + b2\n",
    "            probs = f.softmax(logits, dim=1)\n",
    "            ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "            out.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "            if ix == 0:\n",
    "                break\n",
    "\n",
    "\n",
    "        print(\"\".join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ee9d24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3994663a",
   "metadata": {},
   "source": [
    "## E01: Tune the hyperparameters of the training to beat my best validation loss of 2.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "65aec5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(emb_dim, context_len, nhidden, batch_size):\n",
    "    # Network initializations\n",
    "    emb_dim = 10 # dimension of each character embedding\n",
    "    context_len = 3 # number or input characters / context lenghts\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    C = torch.randn((27,emb_dim), generator=g) # 26 alphabets + 1 ('.') character \n",
    "    W1 = torch.randn((emb_dim*context_len, nhidden), generator=g) # 3*2 (3 characters of 2 dimension each)\n",
    "    b1 = torch.randn(nhidden, generator=g)\n",
    "    W2 = torch.randn((nhidden,27), generator=g)\n",
    "    b2 = torch.randn(27, generator=g)\n",
    "    parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "    print(\"Number of parameters: \", sum(p.nelement() for p in parameters))\n",
    "\n",
    "    for p in parameters:\n",
    "        p.requires_grad = True\n",
    "\n",
    "\n",
    "    def forward_pass(X, Y):\n",
    "        # Forward pass\n",
    "        emb = C[X]  # (32, 3, 2)\n",
    "        h = torch.tanh(emb.view(-1,emb_dim*context_len) @ W1 + b1) # (32, 100)\n",
    "        logits = h @ W2 + b2    # (32, 27)\n",
    "        loss = f.cross_entropy(logits,Y)\n",
    "        return loss\n",
    "\n",
    "    EPOCHS = 200_000     # number of training iterations\n",
    "    BATCH_SIZE = batch_size # mini-batch size\n",
    "\n",
    "\n",
    "    lrei = []\n",
    "    lossi =[]\n",
    "    stepi =[]\n",
    "\n",
    "    # training loop\n",
    "    for i in range(EPOCHS):\n",
    "        # mini-batch\n",
    "        ix = torch.randint(0, Xtr.shape[0], (BATCH_SIZE,))\n",
    "        \n",
    "        # Forward pass\n",
    "        loss = forward_pass(Xtr[ix], Ytr[ix])\n",
    "\n",
    "        # backward pass\n",
    "        for p in parameters: \n",
    "            p.grad = None # set zero-gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update\n",
    "        lr = 0.1 if i < 100_000 else 0.01 if i < 150_000 else 0.001#lrs[i]\n",
    "        for p in parameters:\n",
    "            p.data += -lr*p.grad\n",
    "\n",
    "        # # track-stats\n",
    "        # lrei.append(lre[i])\n",
    "        stepi.append(i)\n",
    "        lossi.append(loss.log10().item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Train loss\n",
    "        tr_loss = forward_pass(Xtr, Ytr).item()\n",
    "        print(f\"{tr_loss=}\")\n",
    "\n",
    "        # Dev loss\n",
    "        dev_loss = forward_pass(Xdev, Ydev).item()\n",
    "        print(f\"{dev_loss=}\")\n",
    "\n",
    "    \n",
    "    return tr_loss, dev_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4119f6b",
   "metadata": {},
   "source": [
    "Hyper parameter Tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "f5c1873b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with e=10 c=3 h=300 b=8\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.1629085540771484\n",
      "dev_loss=2.202441453933716\n",
      "Best Loss found: 2.202441453933716\n",
      "====================================================================================================\n",
      "Training model with e=10 c=3 h=300 b=16\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.1554336547851562\n",
      "dev_loss=2.192016124725342\n",
      "Best Loss found: 2.192016124725342\n",
      "====================================================================================================\n",
      "Training model with e=10 c=3 h=300 b=32\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.093942642211914\n",
      "dev_loss=2.1461846828460693\n",
      "Best Loss found: 2.1461846828460693\n",
      "====================================================================================================\n",
      "Training model with e=10 c=3 h=300 b=64\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.097217559814453\n",
      "dev_loss=2.1570279598236084\n",
      "====================================================================================================\n",
      "Training model with e=10 c=3 h=300 b=128\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.1088707447052\n",
      "dev_loss=2.1613714694976807\n",
      "====================================================================================================\n",
      "Training model with e=10 c=3 h=400 b=8\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.1462788581848145\n",
      "dev_loss=2.198143482208252\n",
      "====================================================================================================\n",
      "Training model with e=10 c=3 h=400 b=16\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.1166934967041016\n",
      "dev_loss=2.1839492321014404\n",
      "====================================================================================================\n",
      "Training model with e=10 c=3 h=400 b=32\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.087829828262329\n",
      "dev_loss=2.1393301486968994\n",
      "Best Loss found: 2.1393301486968994\n",
      "====================================================================================================\n",
      "Training model with e=10 c=3 h=400 b=64\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.0831024646759033\n",
      "dev_loss=2.15645170211792\n",
      "====================================================================================================\n",
      "Training model with e=10 c=3 h=400 b=128\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.0757360458374023\n",
      "dev_loss=2.137221336364746\n",
      "Best Loss found: 2.137221336364746\n",
      "====================================================================================================\n",
      "Training model with e=10 c=3 h=500 b=8\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.137746810913086\n",
      "dev_loss=2.203895330429077\n",
      "====================================================================================================\n",
      "Training model with e=10 c=3 h=500 b=16\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.1451289653778076\n",
      "dev_loss=2.1923720836639404\n",
      "====================================================================================================\n",
      "Training model with e=10 c=3 h=500 b=32\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.0943474769592285\n",
      "dev_loss=2.1739039421081543\n",
      "====================================================================================================\n",
      "Training model with e=10 c=3 h=500 b=64\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.0671558380126953\n",
      "dev_loss=2.1592209339141846\n",
      "====================================================================================================\n",
      "Training model with e=10 c=3 h=500 b=128\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.066110610961914\n",
      "dev_loss=2.1666676998138428\n",
      "====================================================================================================\n",
      "Training model with e=10 c=3 h=1000 b=8\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.25866961479187\n",
      "dev_loss=2.395359992980957\n",
      "====================================================================================================\n",
      "Training model with e=10 c=3 h=1000 b=16\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.1484272480010986\n",
      "dev_loss=2.273524761199951\n",
      "====================================================================================================\n",
      "Training model with e=10 c=3 h=1000 b=32\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.0303211212158203\n",
      "dev_loss=2.197239637374878\n",
      "====================================================================================================\n",
      "Training model with e=10 c=3 h=1000 b=64\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.0147078037261963\n",
      "dev_loss=2.179988145828247\n",
      "====================================================================================================\n",
      "Training model with e=10 c=3 h=1000 b=128\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.001335620880127\n",
      "dev_loss=2.19693660736084\n",
      "====================================================================================================\n",
      "Training model with e=10 c=4 h=300 b=8\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.1394035816192627\n",
      "dev_loss=2.1818904876708984\n",
      "====================================================================================================\n",
      "Training model with e=10 c=4 h=300 b=16\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.143545389175415\n",
      "dev_loss=2.1833364963531494\n",
      "====================================================================================================\n",
      "Training model with e=10 c=4 h=300 b=32\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.1075873374938965\n",
      "dev_loss=2.162583827972412\n",
      "====================================================================================================\n",
      "Training model with e=10 c=4 h=300 b=64\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.1048755645751953\n",
      "dev_loss=2.1647517681121826\n",
      "====================================================================================================\n",
      "Training model with e=10 c=4 h=300 b=128\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.1035096645355225\n",
      "dev_loss=2.1645829677581787\n",
      "====================================================================================================\n",
      "Training model with e=10 c=4 h=400 b=8\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.1435165405273438\n",
      "dev_loss=2.1902098655700684\n",
      "====================================================================================================\n",
      "Training model with e=10 c=4 h=400 b=16\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.0752696990966797\n",
      "dev_loss=2.1521942615509033\n",
      "====================================================================================================\n",
      "Training model with e=10 c=4 h=400 b=32\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.084221363067627\n",
      "dev_loss=2.1516168117523193\n",
      "====================================================================================================\n",
      "Training model with e=10 c=4 h=400 b=64\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.078075885772705\n",
      "dev_loss=2.1518585681915283\n",
      "====================================================================================================\n",
      "Training model with e=10 c=4 h=400 b=128\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.093505382537842\n",
      "dev_loss=2.142658233642578\n",
      "====================================================================================================\n",
      "Training model with e=10 c=4 h=500 b=8\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.1653599739074707\n",
      "dev_loss=2.2178544998168945\n",
      "====================================================================================================\n",
      "Training model with e=10 c=4 h=500 b=16\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.142423391342163\n",
      "dev_loss=2.193143606185913\n",
      "====================================================================================================\n",
      "Training model with e=10 c=4 h=500 b=32\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.101306438446045\n",
      "dev_loss=2.1666042804718018\n",
      "====================================================================================================\n",
      "Training model with e=10 c=4 h=500 b=64\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.0637762546539307\n",
      "dev_loss=2.1696884632110596\n",
      "====================================================================================================\n",
      "Training model with e=10 c=4 h=500 b=128\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.0709948539733887\n",
      "dev_loss=2.1586341857910156\n",
      "====================================================================================================\n",
      "Training model with e=10 c=4 h=1000 b=8\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.2226767539978027\n",
      "dev_loss=2.37874698638916\n",
      "====================================================================================================\n",
      "Training model with e=10 c=4 h=1000 b=16\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.123934745788574\n",
      "dev_loss=2.264735698699951\n",
      "====================================================================================================\n",
      "Training model with e=10 c=4 h=1000 b=32\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.0436246395111084\n",
      "dev_loss=2.179164171218872\n",
      "====================================================================================================\n",
      "Training model with e=10 c=4 h=1000 b=64\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.0029306411743164\n",
      "dev_loss=2.1661829948425293\n",
      "====================================================================================================\n",
      "Training model with e=10 c=4 h=1000 b=128\n",
      "Number of parameters:  58297\n",
      "tr_loss=1.984928011894226\n",
      "dev_loss=2.2010204792022705\n",
      "====================================================================================================\n",
      "Training model with e=10 c=5 h=300 b=8\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.143120050430298\n",
      "dev_loss=2.188331365585327\n",
      "====================================================================================================\n",
      "Training model with e=10 c=5 h=300 b=16\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.126932144165039\n",
      "dev_loss=2.1813321113586426\n",
      "====================================================================================================\n",
      "Training model with e=10 c=5 h=300 b=32\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.1069815158843994\n",
      "dev_loss=2.156625270843506\n",
      "====================================================================================================\n",
      "Training model with e=10 c=5 h=300 b=64\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.1003012657165527\n",
      "dev_loss=2.1693203449249268\n",
      "====================================================================================================\n",
      "Training model with e=10 c=5 h=300 b=128\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.1013875007629395\n",
      "dev_loss=2.162447690963745\n",
      "====================================================================================================\n",
      "Training model with e=10 c=5 h=400 b=8\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.123765707015991\n",
      "dev_loss=2.192899465560913\n",
      "====================================================================================================\n",
      "Training model with e=10 c=5 h=400 b=16\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.103074789047241\n",
      "dev_loss=2.167602777481079\n",
      "====================================================================================================\n",
      "Training model with e=10 c=5 h=400 b=32\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.0665507316589355\n",
      "dev_loss=2.1524741649627686\n",
      "====================================================================================================\n",
      "Training model with e=10 c=5 h=400 b=64\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.070964813232422\n",
      "dev_loss=2.1401689052581787\n",
      "====================================================================================================\n",
      "Training model with e=10 c=5 h=400 b=128\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.0820250511169434\n",
      "dev_loss=2.1520144939422607\n",
      "====================================================================================================\n",
      "Training model with e=10 c=5 h=500 b=8\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.122981071472168\n",
      "dev_loss=2.195082664489746\n",
      "====================================================================================================\n",
      "Training model with e=10 c=5 h=500 b=16\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.1458146572113037\n",
      "dev_loss=2.1921024322509766\n",
      "====================================================================================================\n",
      "Training model with e=10 c=5 h=500 b=32\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.1150081157684326\n",
      "dev_loss=2.17374587059021\n",
      "====================================================================================================\n",
      "Training model with e=10 c=5 h=500 b=64\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.0639407634735107\n",
      "dev_loss=2.1539673805236816\n",
      "====================================================================================================\n",
      "Training model with e=10 c=5 h=500 b=128\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.075460910797119\n",
      "dev_loss=2.1598098278045654\n",
      "====================================================================================================\n",
      "Training model with e=10 c=5 h=1000 b=8\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.247903347015381\n",
      "dev_loss=2.398733139038086\n",
      "====================================================================================================\n",
      "Training model with e=10 c=5 h=1000 b=16\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.1296119689941406\n",
      "dev_loss=2.2436766624450684\n",
      "====================================================================================================\n",
      "Training model with e=10 c=5 h=1000 b=32\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.05588436126709\n",
      "dev_loss=2.176403760910034\n",
      "====================================================================================================\n",
      "Training model with e=10 c=5 h=1000 b=64\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.006574869155884\n",
      "dev_loss=2.1766350269317627\n",
      "====================================================================================================\n",
      "Training model with e=10 c=5 h=1000 b=128\n",
      "Number of parameters:  58297\n",
      "tr_loss=1.9916186332702637\n",
      "dev_loss=2.175759792327881\n",
      "====================================================================================================\n",
      "Training model with e=27 c=3 h=300 b=8\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.1619653701782227\n",
      "dev_loss=2.1985561847686768\n",
      "====================================================================================================\n",
      "Training model with e=27 c=3 h=300 b=16\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.1189212799072266\n",
      "dev_loss=2.156017541885376\n",
      "====================================================================================================\n",
      "Training model with e=27 c=3 h=300 b=32\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.108133554458618\n",
      "dev_loss=2.1666884422302246\n",
      "====================================================================================================\n",
      "Training model with e=27 c=3 h=300 b=64\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.093581438064575\n",
      "dev_loss=2.143704414367676\n",
      "====================================================================================================\n",
      "Training model with e=27 c=3 h=300 b=128\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.1036367416381836\n",
      "dev_loss=2.1594254970550537\n",
      "====================================================================================================\n",
      "Training model with e=27 c=3 h=400 b=8\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.1558611392974854\n",
      "dev_loss=2.2113101482391357\n",
      "====================================================================================================\n",
      "Training model with e=27 c=3 h=400 b=16\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.110731601715088\n",
      "dev_loss=2.173032760620117\n",
      "====================================================================================================\n",
      "Training model with e=27 c=3 h=400 b=32\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.0615808963775635\n",
      "dev_loss=2.1512207984924316\n",
      "====================================================================================================\n",
      "Training model with e=27 c=3 h=400 b=64\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.0818076133728027\n",
      "dev_loss=2.1526689529418945\n",
      "====================================================================================================\n",
      "Training model with e=27 c=3 h=400 b=128\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.0864481925964355\n",
      "dev_loss=2.1491265296936035\n",
      "====================================================================================================\n",
      "Training model with e=27 c=3 h=500 b=8\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.178436517715454\n",
      "dev_loss=2.2358005046844482\n",
      "====================================================================================================\n",
      "Training model with e=27 c=3 h=500 b=16\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.161581516265869\n",
      "dev_loss=2.200721263885498\n",
      "====================================================================================================\n",
      "Training model with e=27 c=3 h=500 b=32\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.0443642139434814\n",
      "dev_loss=2.1378371715545654\n",
      "====================================================================================================\n",
      "Training model with e=27 c=3 h=500 b=64\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.0713682174682617\n",
      "dev_loss=2.1462762355804443\n",
      "====================================================================================================\n",
      "Training model with e=27 c=3 h=500 b=128\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.074815273284912\n",
      "dev_loss=2.1570351123809814\n",
      "====================================================================================================\n",
      "Training model with e=27 c=3 h=1000 b=8\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.2365763187408447\n",
      "dev_loss=2.379687547683716\n",
      "====================================================================================================\n",
      "Training model with e=27 c=3 h=1000 b=16\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.135586738586426\n",
      "dev_loss=2.246093511581421\n",
      "====================================================================================================\n",
      "Training model with e=27 c=3 h=1000 b=32\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.057420015335083\n",
      "dev_loss=2.1839399337768555\n",
      "====================================================================================================\n",
      "Training model with e=27 c=3 h=1000 b=64\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.0265631675720215\n",
      "dev_loss=2.1660900115966797\n",
      "====================================================================================================\n",
      "Training model with e=27 c=3 h=1000 b=128\n",
      "Number of parameters:  58297\n",
      "tr_loss=1.987917184829712\n",
      "dev_loss=2.2037177085876465\n",
      "====================================================================================================\n",
      "Training model with e=27 c=4 h=300 b=8\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.1839499473571777\n",
      "dev_loss=2.2177305221557617\n",
      "====================================================================================================\n",
      "Training model with e=27 c=4 h=300 b=16\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.1354870796203613\n",
      "dev_loss=2.1817076206207275\n",
      "====================================================================================================\n",
      "Training model with e=27 c=4 h=300 b=32\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.1246891021728516\n",
      "dev_loss=2.1753714084625244\n",
      "====================================================================================================\n",
      "Training model with e=27 c=4 h=300 b=64\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.112743616104126\n",
      "dev_loss=2.1508140563964844\n",
      "====================================================================================================\n",
      "Training model with e=27 c=4 h=300 b=128\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.1053311824798584\n",
      "dev_loss=2.168372392654419\n",
      "====================================================================================================\n",
      "Training model with e=27 c=4 h=400 b=8\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.146552801132202\n",
      "dev_loss=2.1932783126831055\n",
      "====================================================================================================\n",
      "Training model with e=27 c=4 h=400 b=16\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.1542396545410156\n",
      "dev_loss=2.2065577507019043\n",
      "====================================================================================================\n",
      "Training model with e=27 c=4 h=400 b=32\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.0939035415649414\n",
      "dev_loss=2.1577141284942627\n",
      "====================================================================================================\n",
      "Training model with e=27 c=4 h=400 b=64\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.0774929523468018\n",
      "dev_loss=2.1537508964538574\n",
      "====================================================================================================\n",
      "Training model with e=27 c=4 h=400 b=128\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.0818839073181152\n",
      "dev_loss=2.1433346271514893\n",
      "====================================================================================================\n",
      "Training model with e=27 c=4 h=500 b=8\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.1498935222625732\n",
      "dev_loss=2.206101655960083\n",
      "====================================================================================================\n",
      "Training model with e=27 c=4 h=500 b=16\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.0962471961975098\n",
      "dev_loss=2.1723363399505615\n",
      "====================================================================================================\n",
      "Training model with e=27 c=4 h=500 b=32\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.065380573272705\n",
      "dev_loss=2.160893201828003\n",
      "====================================================================================================\n",
      "Training model with e=27 c=4 h=500 b=64\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.06032657623291\n",
      "dev_loss=2.159336805343628\n",
      "====================================================================================================\n",
      "Training model with e=27 c=4 h=500 b=128\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.0779776573181152\n",
      "dev_loss=2.145249128341675\n",
      "====================================================================================================\n",
      "Training model with e=27 c=4 h=1000 b=8\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.198732376098633\n",
      "dev_loss=2.3558361530303955\n",
      "====================================================================================================\n",
      "Training model with e=27 c=4 h=1000 b=16\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.1736128330230713\n",
      "dev_loss=2.2767937183380127\n",
      "====================================================================================================\n",
      "Training model with e=27 c=4 h=1000 b=32\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.0595083236694336\n",
      "dev_loss=2.186037302017212\n",
      "====================================================================================================\n",
      "Training model with e=27 c=4 h=1000 b=64\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.0227468013763428\n",
      "dev_loss=2.1941962242126465\n",
      "====================================================================================================\n",
      "Training model with e=27 c=4 h=1000 b=128\n",
      "Number of parameters:  58297\n",
      "tr_loss=1.98810613155365\n",
      "dev_loss=2.1962015628814697\n",
      "====================================================================================================\n",
      "Training model with e=27 c=5 h=300 b=8\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.170219659805298\n",
      "dev_loss=2.2058115005493164\n",
      "====================================================================================================\n",
      "Training model with e=27 c=5 h=300 b=16\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.126837730407715\n",
      "dev_loss=2.1742639541625977\n",
      "====================================================================================================\n",
      "Training model with e=27 c=5 h=300 b=32\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.1016604900360107\n",
      "dev_loss=2.1557629108428955\n",
      "====================================================================================================\n",
      "Training model with e=27 c=5 h=300 b=64\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.106285572052002\n",
      "dev_loss=2.167729377746582\n",
      "====================================================================================================\n",
      "Training model with e=27 c=5 h=300 b=128\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.1018285751342773\n",
      "dev_loss=2.1588122844696045\n",
      "====================================================================================================\n",
      "Training model with e=27 c=5 h=400 b=8\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.1552040576934814\n",
      "dev_loss=2.2026681900024414\n",
      "====================================================================================================\n",
      "Training model with e=27 c=5 h=400 b=16\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.104025363922119\n",
      "dev_loss=2.1656460762023926\n",
      "====================================================================================================\n",
      "Training model with e=27 c=5 h=400 b=32\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.0982112884521484\n",
      "dev_loss=2.161207914352417\n",
      "====================================================================================================\n",
      "Training model with e=27 c=5 h=400 b=64\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.0777406692504883\n",
      "dev_loss=2.1437225341796875\n",
      "====================================================================================================\n",
      "Training model with e=27 c=5 h=400 b=128\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.093324661254883\n",
      "dev_loss=2.1437556743621826\n",
      "====================================================================================================\n",
      "Training model with e=27 c=5 h=500 b=8\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.1504504680633545\n",
      "dev_loss=2.225571870803833\n",
      "====================================================================================================\n",
      "Training model with e=27 c=5 h=500 b=16\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.140475273132324\n",
      "dev_loss=2.1920862197875977\n",
      "====================================================================================================\n",
      "Training model with e=27 c=5 h=500 b=32\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.0899035930633545\n",
      "dev_loss=2.1690876483917236\n",
      "====================================================================================================\n",
      "Training model with e=27 c=5 h=500 b=64\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.0808651447296143\n",
      "dev_loss=2.1672873497009277\n",
      "====================================================================================================\n",
      "Training model with e=27 c=5 h=500 b=128\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.0774388313293457\n",
      "dev_loss=2.149200201034546\n",
      "====================================================================================================\n",
      "Training model with e=27 c=5 h=1000 b=8\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.2148184776306152\n",
      "dev_loss=2.355034589767456\n",
      "====================================================================================================\n",
      "Training model with e=27 c=5 h=1000 b=16\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.117034673690796\n",
      "dev_loss=2.264320135116577\n",
      "====================================================================================================\n",
      "Training model with e=27 c=5 h=1000 b=32\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.0435397624969482\n",
      "dev_loss=2.181380271911621\n",
      "====================================================================================================\n",
      "Training model with e=27 c=5 h=1000 b=64\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.004997730255127\n",
      "dev_loss=2.1711745262145996\n",
      "====================================================================================================\n",
      "Training model with e=27 c=5 h=1000 b=128\n",
      "Number of parameters:  58297\n",
      "tr_loss=1.991538166999817\n",
      "dev_loss=2.1872007846832275\n",
      "====================================================================================================\n",
      "Training model with e=50 c=3 h=300 b=8\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.1725499629974365\n",
      "dev_loss=2.201317071914673\n",
      "====================================================================================================\n",
      "Training model with e=50 c=3 h=300 b=16\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.1095714569091797\n",
      "dev_loss=2.1593635082244873\n",
      "====================================================================================================\n",
      "Training model with e=50 c=3 h=300 b=32\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.1152985095977783\n",
      "dev_loss=2.1777920722961426\n",
      "====================================================================================================\n",
      "Training model with e=50 c=3 h=300 b=64\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.1101372241973877\n",
      "dev_loss=2.165574073791504\n",
      "====================================================================================================\n",
      "Training model with e=50 c=3 h=300 b=128\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.105186700820923\n",
      "dev_loss=2.159961462020874\n",
      "====================================================================================================\n",
      "Training model with e=50 c=3 h=400 b=8\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.1342098712921143\n",
      "dev_loss=2.189271926879883\n",
      "====================================================================================================\n",
      "Training model with e=50 c=3 h=400 b=16\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.1492109298706055\n",
      "dev_loss=2.1884994506835938\n",
      "====================================================================================================\n",
      "Training model with e=50 c=3 h=400 b=32\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.0806005001068115\n",
      "dev_loss=2.1615891456604004\n",
      "====================================================================================================\n",
      "Training model with e=50 c=3 h=400 b=64\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.0728416442871094\n",
      "dev_loss=2.156139373779297\n",
      "====================================================================================================\n",
      "Training model with e=50 c=3 h=400 b=128\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.085742950439453\n",
      "dev_loss=2.1464102268218994\n",
      "====================================================================================================\n",
      "Training model with e=50 c=3 h=500 b=8\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.1646203994750977\n",
      "dev_loss=2.2251627445220947\n",
      "====================================================================================================\n",
      "Training model with e=50 c=3 h=500 b=16\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.1435606479644775\n",
      "dev_loss=2.204620361328125\n",
      "====================================================================================================\n",
      "Training model with e=50 c=3 h=500 b=32\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.101353406906128\n",
      "dev_loss=2.153324842453003\n",
      "====================================================================================================\n",
      "Training model with e=50 c=3 h=500 b=64\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.0582423210144043\n",
      "dev_loss=2.1424262523651123\n",
      "====================================================================================================\n",
      "Training model with e=50 c=3 h=500 b=128\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.062346935272217\n",
      "dev_loss=2.152710437774658\n",
      "====================================================================================================\n",
      "Training model with e=50 c=3 h=1000 b=8\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.2251110076904297\n",
      "dev_loss=2.3767740726470947\n",
      "====================================================================================================\n",
      "Training model with e=50 c=3 h=1000 b=16\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.140702486038208\n",
      "dev_loss=2.2581472396850586\n",
      "====================================================================================================\n",
      "Training model with e=50 c=3 h=1000 b=32\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.0736002922058105\n",
      "dev_loss=2.174703359603882\n",
      "====================================================================================================\n",
      "Training model with e=50 c=3 h=1000 b=64\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.0265772342681885\n",
      "dev_loss=2.1628804206848145\n",
      "====================================================================================================\n",
      "Training model with e=50 c=3 h=1000 b=128\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.0063648223876953\n",
      "dev_loss=2.1998252868652344\n",
      "====================================================================================================\n",
      "Training model with e=50 c=4 h=300 b=8\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.1770973205566406\n",
      "dev_loss=2.2114694118499756\n",
      "====================================================================================================\n",
      "Training model with e=50 c=4 h=300 b=16\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.126821279525757\n",
      "dev_loss=2.1831867694854736\n",
      "====================================================================================================\n",
      "Training model with e=50 c=4 h=300 b=32\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.090688943862915\n",
      "dev_loss=2.148512125015259\n",
      "====================================================================================================\n",
      "Training model with e=50 c=4 h=300 b=64\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.0982446670532227\n",
      "dev_loss=2.1633596420288086\n",
      "====================================================================================================\n",
      "Training model with e=50 c=4 h=300 b=128\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.1054725646972656\n",
      "dev_loss=2.153751850128174\n",
      "====================================================================================================\n",
      "Training model with e=50 c=4 h=400 b=8\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.1346282958984375\n",
      "dev_loss=2.195173501968384\n",
      "====================================================================================================\n",
      "Training model with e=50 c=4 h=400 b=16\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.108978509902954\n",
      "dev_loss=2.17702054977417\n",
      "====================================================================================================\n",
      "Training model with e=50 c=4 h=400 b=32\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.0972952842712402\n",
      "dev_loss=2.1639227867126465\n",
      "====================================================================================================\n",
      "Training model with e=50 c=4 h=400 b=64\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.078185796737671\n",
      "dev_loss=2.169647455215454\n",
      "====================================================================================================\n",
      "Training model with e=50 c=4 h=400 b=128\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.0808515548706055\n",
      "dev_loss=2.1525938510894775\n",
      "====================================================================================================\n",
      "Training model with e=50 c=4 h=500 b=8\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.1651082038879395\n",
      "dev_loss=2.222637176513672\n",
      "====================================================================================================\n",
      "Training model with e=50 c=4 h=500 b=16\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.0818686485290527\n",
      "dev_loss=2.166849136352539\n",
      "====================================================================================================\n",
      "Training model with e=50 c=4 h=500 b=32\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.083684206008911\n",
      "dev_loss=2.159874439239502\n",
      "====================================================================================================\n",
      "Training model with e=50 c=4 h=500 b=64\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.0616543292999268\n",
      "dev_loss=2.167792797088623\n",
      "====================================================================================================\n",
      "Training model with e=50 c=4 h=500 b=128\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.0681371688842773\n",
      "dev_loss=2.1578845977783203\n",
      "====================================================================================================\n",
      "Training model with e=50 c=4 h=1000 b=8\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.263201951980591\n",
      "dev_loss=2.4240474700927734\n",
      "====================================================================================================\n",
      "Training model with e=50 c=4 h=1000 b=16\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.142634153366089\n",
      "dev_loss=2.254382371902466\n",
      "====================================================================================================\n",
      "Training model with e=50 c=4 h=1000 b=32\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.043513059616089\n",
      "dev_loss=2.189068555831909\n",
      "====================================================================================================\n",
      "Training model with e=50 c=4 h=1000 b=64\n",
      "Number of parameters:  58297\n",
      "tr_loss=1.9902164936065674\n",
      "dev_loss=2.181185483932495\n",
      "====================================================================================================\n",
      "Training model with e=50 c=4 h=1000 b=128\n",
      "Number of parameters:  58297\n",
      "tr_loss=1.999504804611206\n",
      "dev_loss=2.1933207511901855\n",
      "====================================================================================================\n",
      "Training model with e=50 c=5 h=300 b=8\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.1516072750091553\n",
      "dev_loss=2.1850032806396484\n",
      "====================================================================================================\n",
      "Training model with e=50 c=5 h=300 b=16\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.140531301498413\n",
      "dev_loss=2.1741950511932373\n",
      "====================================================================================================\n",
      "Training model with e=50 c=5 h=300 b=32\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.103409767150879\n",
      "dev_loss=2.1628167629241943\n",
      "====================================================================================================\n",
      "Training model with e=50 c=5 h=300 b=64\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.0977025032043457\n",
      "dev_loss=2.1515419483184814\n",
      "====================================================================================================\n",
      "Training model with e=50 c=5 h=300 b=128\n",
      "Number of parameters:  17697\n",
      "tr_loss=2.101641893386841\n",
      "dev_loss=2.1517744064331055\n",
      "====================================================================================================\n",
      "Training model with e=50 c=5 h=400 b=8\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.1303515434265137\n",
      "dev_loss=2.1903202533721924\n",
      "====================================================================================================\n",
      "Training model with e=50 c=5 h=400 b=16\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.106041431427002\n",
      "dev_loss=2.168490409851074\n",
      "====================================================================================================\n",
      "Training model with e=50 c=5 h=400 b=32\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.0972537994384766\n",
      "dev_loss=2.1695351600646973\n",
      "====================================================================================================\n",
      "Training model with e=50 c=5 h=400 b=64\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.074881076812744\n",
      "dev_loss=2.1547720432281494\n",
      "====================================================================================================\n",
      "Training model with e=50 c=5 h=400 b=128\n",
      "Number of parameters:  23497\n",
      "tr_loss=2.087305784225464\n",
      "dev_loss=2.147310733795166\n",
      "====================================================================================================\n",
      "Training model with e=50 c=5 h=500 b=8\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.1501924991607666\n",
      "dev_loss=2.2191975116729736\n",
      "====================================================================================================\n",
      "Training model with e=50 c=5 h=500 b=16\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.1431055068969727\n",
      "dev_loss=2.1989822387695312\n",
      "====================================================================================================\n",
      "Training model with e=50 c=5 h=500 b=32\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.0943832397460938\n",
      "dev_loss=2.172330141067505\n",
      "====================================================================================================\n",
      "Training model with e=50 c=5 h=500 b=64\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.0610930919647217\n",
      "dev_loss=2.178182363510132\n",
      "====================================================================================================\n",
      "Training model with e=50 c=5 h=500 b=128\n",
      "Number of parameters:  29297\n",
      "tr_loss=2.0780019760131836\n",
      "dev_loss=2.145439863204956\n",
      "====================================================================================================\n",
      "Training model with e=50 c=5 h=1000 b=8\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.1890299320220947\n",
      "dev_loss=2.352759599685669\n",
      "====================================================================================================\n",
      "Training model with e=50 c=5 h=1000 b=16\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.1016316413879395\n",
      "dev_loss=2.2414395809173584\n",
      "====================================================================================================\n",
      "Training model with e=50 c=5 h=1000 b=32\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.074291944503784\n",
      "dev_loss=2.1902923583984375\n",
      "====================================================================================================\n",
      "Training model with e=50 c=5 h=1000 b=64\n",
      "Number of parameters:  58297\n",
      "tr_loss=1.992846131324768\n",
      "dev_loss=2.1871705055236816\n",
      "====================================================================================================\n",
      "Training model with e=50 c=5 h=1000 b=128\n",
      "Number of parameters:  58297\n",
      "tr_loss=2.004875659942627\n",
      "dev_loss=2.1881601810455322\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# hyper paramters\n",
    "emb_dim = [10, 27, 50]\n",
    "context_len = [3, 4, 5]\n",
    "hidden_layers = [300, 400, 500, 1000]\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "# ephocs = [200_000, 300_000]\n",
    "\n",
    "results = []\n",
    "best_loss = torch.inf\n",
    "best_setting = {}\n",
    "\n",
    "for e in emb_dim:\n",
    "    for c in context_len:\n",
    "        for h in hidden_layers:\n",
    "            for b in batch_sizes:\n",
    "                print(f\"Training model with {e=} {c=} {h=} {b=}\")\n",
    "                tr_loss, dev_loss= train_model(e, c, h, b)\n",
    "                results.append({\n",
    "                    'Embedding Dim': e,\n",
    "                    'Context len' : c,\n",
    "                    'Hidden layer': h,\n",
    "                    \"Batch Size\":b,\n",
    "                    \"Train loss\": tr_loss,\n",
    "                    \"Dev Loss\": dev_loss\n",
    "                })\n",
    "\n",
    "                if dev_loss < best_loss:\n",
    "                    print(\"Best Loss found:\", dev_loss)\n",
    "                    best_loss = dev_loss\n",
    "                    best_setting = {\n",
    "                    'Embedding Dim': e,\n",
    "                    'Context len' : c,\n",
    "                    'Hidden layer': h,\n",
    "                    \"Batch Size\":b,\n",
    "                    \"Train loss\": tr_loss,\n",
    "                    \"Dev Loss\": dev_loss\n",
    "                }\n",
    "                print(\"=\"*100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "43f41a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Embedding Dim': 10,\n",
       " 'Context len': 3,\n",
       " 'Hidden layer': 400,\n",
       " 'Batch Size': 128,\n",
       " 'Train loss': 2.0757360458374023,\n",
       " 'Dev Loss': 2.137221336364746}"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723b4ffe",
   "metadata": {},
   "source": [
    "## E02: I was not careful with the intialization of the network in this video. \n",
    "\n",
    "(1) What is the loss you'd get if the predicted probabilities at initialization were perfectly uniform? What loss do we achieve?\n",
    " \n",
    "(2) Can you tune the initialization to get a starting loss that is much more similar to (1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f1791a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.295836925506592"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting perdicted probabilities at initialization perfectly uniform \n",
    "# In other words, the model is expected to predict each character with equal probability.\n",
    "# This is equivalent to the model predicting the logits as 0 for all characters.\n",
    "# The loss in this case is -log(1/27) for each character.\n",
    "-torch.tensor(1/27).log().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dab04b6",
   "metadata": {},
   "source": [
    "Initialize weights and biases of network to obtain the nll loss similar to `3.29`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "62396cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  11897\n",
      "loss.item()=3.297171115875244\n"
     ]
    }
   ],
   "source": [
    "# Network initializations\n",
    "emb_dim = 10 # dimension of each character embedding\n",
    "context_len = 3 # number or input characters / context lenghts\n",
    "nhidden = 200\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27,emb_dim),                   generator=g) # 26 alphabets + 1 ('.') character \n",
    "W1 = torch.randn((emb_dim*context_len, nhidden),generator=g) * 0.01\n",
    "b1 = torch.randn(nhidden,                       generator=g) * 0.01\n",
    "W2 = torch.randn((nhidden,27),                  generator=g) * 0.01\n",
    "b2 = torch.randn(27,                            generator=g) * 0\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "print(\"Number of parameters: \", sum(p.nelement() for p in parameters))\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "\n",
    "def forward_pass(X, Y):\n",
    "    # Forward pass\n",
    "    emb = C[X]  # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1,emb_dim*context_len) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2    # (32, 27)\n",
    "    loss = f.cross_entropy(logits,Y)\n",
    "    return loss\n",
    "\n",
    "EPOCHS = 200_000     # number of training iterations\n",
    "BATCH_SIZE = 32 # mini-batch size\n",
    "\n",
    "\n",
    "lrei = []\n",
    "lossi =[]\n",
    "stepi =[]\n",
    "\n",
    "# training loop\n",
    "for i in range(EPOCHS):\n",
    "    # mini-batch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (BATCH_SIZE,), generator=g)\n",
    "    \n",
    "    # Forward pass\n",
    "    # loss = forward_pass(Xtr[ix], Ytr[ix])\n",
    "    emb = C[Xtr[ix]]  # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1,emb_dim*context_len) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2    # (32, 27)\n",
    "    loss = f.cross_entropy(logits,Ytr[ix])\n",
    "    print(f\"{loss.item()=}\")\n",
    "\n",
    "    # # backward pass\n",
    "    # for p in parameters: \n",
    "    #     p.grad = None # set zero-gradients\n",
    "    # loss.backward()\n",
    "\n",
    "    # # update\n",
    "    # lr = 0.1 if i < 100_000 else 0.01 if i < 150_000 else 0.001#lrs[i]\n",
    "    # for p in parameters:\n",
    "    #     p.data += -lr*p.grad\n",
    "\n",
    "    # # # track-stats\n",
    "    # # lrei.append(lre[i])\n",
    "    # stepi.append(i)\n",
    "    # lossi.append(loss.log10().item())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dc6a3c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0368, 0.0377, 0.0369, 0.0372, 0.0372, 0.0369, 0.0369, 0.0371, 0.0371,\n",
       "        0.0372, 0.0374, 0.0368, 0.0370, 0.0365, 0.0369, 0.0371, 0.0369, 0.0366,\n",
       "        0.0365, 0.0370, 0.0373, 0.0375, 0.0374, 0.0368, 0.0366, 0.0374, 0.0373],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = logits.exp()\n",
    "probs = counts/counts.sum(dim=1, keepdim=True)\n",
    "probs[0] # probabilities of all characters for the first character in the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7401a2",
   "metadata": {},
   "source": [
    "Initializing weights and biases with the Random numbers is not always a good approach. Because this makes the network think some characters are more likely than others. For our problem we know that all 27 characters are equally likely given the previous character at input, so it's better to initialize the network in such a way it starts at predicting equal probabilities for all characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cfa538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9efa6678",
   "metadata": {},
   "source": [
    "# E03: Read the Bengio et al 2003 paper (link above), implement and try any idea from the paper. Did it work?\n",
    "\n",
    "## Connecting Word Features to Output\n",
    "Paper claims the learning of \"linear\" part of mapping from word features to log-probabilities is faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a95c9ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  11897\n",
      "loss.item()=26.46274757385254\n",
      "loss.item()=2.10806941986084\n",
      "loss.item()=2.6410505771636963\n",
      "loss.item()=2.9132485389709473\n",
      "loss.item()=2.111428737640381\n",
      "loss.item()=2.2085177898406982\n",
      "loss.item()=2.648627519607544\n",
      "loss.item()=2.2578189373016357\n",
      "loss.item()=2.3211097717285156\n",
      "loss.item()=2.3930323123931885\n"
     ]
    }
   ],
   "source": [
    "# Network initializations\n",
    "emb_dim = 10 # dimension of each character embedding\n",
    "context_len = 3 # number or input characters / context lenghts\n",
    "nhidden = 200\n",
    "nvocab = 27\n",
    "direct = True\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((nvocab,emb_dim),                   generator=g) # 26 alphabets + 1 ('.') character \n",
    "W1 = torch.randn((emb_dim*context_len, nhidden),    generator=g) #* 0.01\n",
    "b1 = torch.randn(nhidden,                           generator=g) #* 0.01\n",
    "W2 = torch.randn((nhidden,nvocab),                  generator=g) #* 0.01\n",
    "b2 = torch.randn(nvocab,                            generator=g) #* 0\n",
    "Wf = torch.randn((emb_dim*context_len, nvocab),     generator=g) #* 0.01\n",
    "bf = torch.randn(nvocab,                            generator=g) #* 0\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "print(\"Number of parameters: \", sum(p.nelement() for p in parameters))\n",
    "\n",
    "if direct:\n",
    "    parameters.append(Wf)\n",
    "    parameters.append(bf)\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "\n",
    "# def forward_pass(X, Y):\n",
    "#     # Forward pass\n",
    "#     emb = C[X]  # (32, 3, 2)\n",
    "#     print(f\"{emb.shape=}\")\n",
    "#     h = torch.tanh(emb.view(-1,emb_dim*context_len) @ W1 + b1) # (32, 100)\n",
    "#     print(f\"{h.shape=}\")\n",
    "#     logits = h @ W2 + b2    # (32, 27)\n",
    "#     loss = f.cross_entropy(logits,Y)\n",
    "#     return logits, loss\n",
    "\n",
    "EPOCHS = 50_000     # number of training iterations\n",
    "BATCH_SIZE = 32 # mini-batch size\n",
    "\n",
    "\n",
    "lrei = []\n",
    "lossi =[]\n",
    "stepi =[]\n",
    "\n",
    "# training loop\n",
    "for i in range(EPOCHS):\n",
    "    # mini-batch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (BATCH_SIZE,), generator=g)\n",
    "    \n",
    "    # Forward pass\n",
    "    emb = C[Xtr[ix]]  # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1,emb_dim*context_len) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2    # (32, 27)\n",
    "    if direct:\n",
    "        logits += emb.view(-1, emb_dim*context_len)@Wf + bf    # (32, 27)\n",
    "    loss = f.cross_entropy(logits,Ytr[ix])\n",
    "    \n",
    "    if i % (EPOCHS//10) == 0:\n",
    "        print(f\"{loss.item()=}\")\n",
    "\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters: \n",
    "        p.grad = None # set zero-gradients\n",
    "\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 30_000 else 0.01 if i < 40_000 else 0.001#lrs[i]\n",
    "    for p in parameters:\n",
    "        p.data += -lr*p.grad\n",
    "\n",
    "    # # track-stats\n",
    "    # lrei.append(lre[i])\n",
    "    stepi.append(i)\n",
    "    lossi.append(loss.log10().item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "202ad7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1221d4560>]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(stepi,lossi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b016e5da",
   "metadata": {},
   "source": [
    "There is no discernable impact on connecting word features to output. We can only see a very small improvement in the training loss over epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e725c074",
   "metadata": {},
   "source": [
    "## Mix probability model\n",
    "\n",
    "Add smoothened trigram model given in Section 4.1 in the paper. The following works have 2 differnt implementations\n",
    "\n",
    "1. Assume $\\alpha$ values with prior knowledge and weigh the decision of the trigram model and NN model equally $\\lambda=0.5$. This approach is very close to paper, only difference being the $\\alpha\\text{'s}$ are estimated using Expectation Maxmimisation (EM) algorithm in the paper. [link](https://colab.research.google.com/drive/10ukMasBgdV3BeD7MIJKwQUtaCasNulZx?usp=sharing)\n",
    "2. Learn the weights of the trigram model along with the NN model during training [link](https://github.com/sohammistri/makemore/blob/main/part-2-exercise/P3-probability-model-mixing.ipynb)\n",
    "\n",
    "\n",
    "I'm going to implement the 1st approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f2589d",
   "metadata": {},
   "source": [
    "## Build N-grams model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0c0b7865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(words_tr)=25626 len(words_dev)=3203 len(words_test)=3204\n"
     ]
    }
   ],
   "source": [
    "# build ngrams\n",
    "words_tr = words[:n1]\n",
    "words_dev = words[n1:n2]\n",
    "words_test = words[n2:]\n",
    "\n",
    "print(f\"{len(words_tr)=} {len(words_dev)=} {len(words_test)=}\")\n",
    "\n",
    "eps = 1e-3\n",
    "count_unigrams = torch.zeros(27, dtype=torch.int32) + eps\n",
    "count_bigrams = torch.zeros(27, 27, dtype=torch.int32) + eps\n",
    "count_trigrams = torch.zeros(27, 27, 27, dtype=torch.int32) + eps\n",
    "\n",
    "for _w in words_tr:\n",
    "    w = ['.'] + list(_w) + ['.']\n",
    "    for i, ch in enumerate(w):\n",
    "        ix = stoi[ch]\n",
    "        count_unigrams[ix] += 1\n",
    "        if i < 1:\n",
    "            continue\n",
    "        count_bigrams[stoi[w[i-1]], ix] += 1\n",
    "        if i < 2:\n",
    "            continue\n",
    "        count_trigrams[stoi[w[i-2]], stoi[w[i-1]], ix] += 1\n",
    "\n",
    "\n",
    "prob_unigrams = count_unigrams/count_unigrams.sum()\n",
    "prob_bigrams = count_bigrams/count_bigrams.sum(dim=1, keepdim=True)\n",
    "prob_trigrams = count_trigrams/count_trigrams.sum(dim=2, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "53e32f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set alphas (interpolation weights)\n",
    "alpha_3 = 0.7 # trigram weight\n",
    "alpha_2 = 0.2 # bigram weight\n",
    "alpha_1 = 0.1 # unigram weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "02f48538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothend trigram model\n",
    "prob_trigrams_smoothend = alpha_1 * prob_unigrams + alpha_2 * prob_bigrams + alpha_3 * prob_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e6035efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27, 27])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_trigrams_smoothend.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ae431f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "09715200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  11897\n",
      "loss.item()=2.3948566913604736\n",
      "loss.item()=2.3601903915405273\n",
      "loss.item()=1.9544622898101807\n",
      "loss.item()=2.470388889312744\n",
      "loss.item()=2.2870399951934814\n",
      "loss.item()=1.9577383995056152\n",
      "loss.item()=1.96755051612854\n",
      "loss.item()=2.362408399581909\n",
      "loss.item()=1.9106603860855103\n",
      "loss.item()=2.0390048027038574\n"
     ]
    }
   ],
   "source": [
    "# Network initializations\n",
    "emb_dim = 10 # dimension of each character embedding\n",
    "context_len = 3 # number or input characters / context lenghts\n",
    "nhidden = 200\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27,emb_dim),                   generator=g) # 26 alphabets + 1 ('.') character \n",
    "W1 = torch.randn((emb_dim*context_len, nhidden),generator=g) * 0.01\n",
    "b1 = torch.randn(nhidden,                       generator=g) * 0.01\n",
    "W2 = torch.randn((nhidden,27),                  generator=g) * 0.01\n",
    "b2 = torch.randn(27,                            generator=g) * 0\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "print(\"Number of parameters: \", sum(p.nelement() for p in parameters))\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "EPOCHS = 200_000     # number of training iterations\n",
    "BATCH_SIZE = 32 # mini-batch size\n",
    "\n",
    "\n",
    "lrei = []\n",
    "lossi =[]\n",
    "stepi =[]\n",
    "alpha = 0.5\n",
    "\n",
    "# training loop\n",
    "for i in range(EPOCHS):\n",
    "    # mini-batch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (BATCH_SIZE,), generator=g)\n",
    "    \n",
    "    # Forward pass\n",
    "    # loss = forward_pass(Xtr[ix], Ytr[ix])\n",
    "    emb = C[Xtr[ix]]  # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1,emb_dim*context_len) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2    # (32, 27)\n",
    "    probs = f.softmax(logits, dim=1)\n",
    "\n",
    "    # get prob distribution of last two characters in context inorder to predict next char\n",
    "    trigram_probs = prob_trigrams_smoothend[Xtr[ix][:, -2], Xtr[ix][:, -1], :] \n",
    "\n",
    "    comb_prob = alpha*probs + (1-alpha)*trigram_probs\n",
    "    \n",
    "    # negative log likelihood loss\n",
    "    loss = -comb_prob[torch.arange(BATCH_SIZE), Ytr[ix]].log().mean()\n",
    "    # loss = f.cross_entropy(logits,Ytr[ix])\n",
    "    if i % (EPOCHS//10) == 0:\n",
    "        print(f\"{loss.item()=}\")\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters: \n",
    "        p.grad = None # set zero-gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100_000 else 0.01 if i < 150_000 else 0.001#lrs[i]\n",
    "    for p in parameters:\n",
    "        p.data += -lr*p.grad\n",
    "\n",
    "    # # track-stats\n",
    "    # lrei.append(lre[i])\n",
    "    stepi.append(i)\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "aa611201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr_loss=tensor(2.1147)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # Train loss\n",
    "    # Forward pass\n",
    "    emb = C[Xtr]  # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1,emb_dim*context_len) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2    # (32, 27)\n",
    "    probs = f.softmax(logits, dim=1)\n",
    "    # get prob distribution of last two characters in context inorder to predict next char\n",
    "    trigram_probs = prob_trigrams_smoothend[Xtr[:, -2], Xtr[:, -1], :] \n",
    "    comb_prob = alpha*probs + (1-alpha)*trigram_probs\n",
    "    tr_loss = -comb_prob[torch.arange(len(Xtr)), Ytr].log().mean()# negative log likelihood loss\n",
    "    print(f\"{tr_loss=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "18f0dfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_loss=tensor(2.1487)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # Dev loss\n",
    "    # Forward pass\n",
    "    emb = C[Xdev]  # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(-1,emb_dim*context_len) @ W1 + b1) # (32, 100)\n",
    "    logits = h @ W2 + b2    # (32, 27)\n",
    "    probs = f.softmax(logits, dim=1)\n",
    "    # get prob distribution of last two characters in context inorder to predict next char\n",
    "    trigram_probs = prob_trigrams_smoothend[Xdev[:, -2], Xdev[:, -1], :] \n",
    "    comb_prob = alpha*probs + (1-alpha)*trigram_probs\n",
    "    dev_loss = -comb_prob[torch.arange(len(Xdev)), Ydev].log().mean()# negative log likelihood loss\n",
    "    print(f\"{dev_loss=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d41ab1",
   "metadata": {},
   "source": [
    "## Sample from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "121563aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carman.\n",
      "amelilli.\n",
      "kimleige.\n",
      "tyrenley.\n",
      "kenrah.\n",
      "bradhery.\n",
      "charqeig.\n",
      "ramari.\n",
      "chaily.\n",
      "kaleig.\n",
      "daymondi.\n",
      "desian.\n",
      "sulie.\n",
      "alvin.\n",
      "aura.\n",
      "elo.\n",
      "dearistin.\n",
      "jenni.\n",
      "sabella.\n",
      "cora.\n"
     ]
    }
   ],
   "source": [
    "# sample from the network model\n",
    "with torch.no_grad():\n",
    "    g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "    for _ in range(20):\n",
    "        out = []\n",
    "        context = [0] * context_len # initialize with all ...\n",
    "        while True:\n",
    "            # forward pass\n",
    "            emb = C[torch.tensor([context])] # 1, 3, 10\n",
    "            h = torch.tanh(emb.view(-1, emb_dim*context_len)@W1 + b1)\n",
    "            logits = h@W2 + b2\n",
    "            probs = f.softmax(logits, dim=1)\n",
    "            # get prob distribution of last two characters in context inorder to predict next char\n",
    "            trigram_probs = prob_trigrams_smoothend[context[-2], context[-1], :] \n",
    "            comb_prob = alpha*probs + (1-alpha)*trigram_probs\n",
    "            ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "            out.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "            if ix == 0:\n",
    "                break\n",
    "\n",
    "\n",
    "        print(\"\".join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148eb432",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
