{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1228d92c",
   "metadata": {},
   "source": [
    "# Activations Gradients and Batch Normalization\n",
    "\n",
    "This notebook is from part 3 of lecture notes from \"Neural Networks: Zero to hero\" series. [Lecture link](https://www.youtube.com/watch?v=P6sfmUTpUmc)\n",
    "\n",
    "This lecture explores the effect of initialization on the gradients and the activations. And how initializations and batch normalizations are helpful in mitigating the issues. \n",
    "\n",
    "1. [Intial training loss and Tanh saturations](#initial-loss-tanh-saturations)\n",
    "2. [Kaiming init](#kaiming-init)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| Initialization   | Initial Loss | Train loss | Val loss | Remarks |\n",
    "|------------------|------------|----------| -- | -- |\n",
    "| Random Normal    | 27.881 | 2.126      | 2.169    | Original |\n",
    "| b2 * 0, W2 * 0.01    | 3.322 | 2.069 | 2.131 | Fix initial loss at output layer |\n",
    "| b2 * 0, W2 * 0.01, b1 * 0.01, W1 * 0.2    | 3.313 | 2.035 | 2.102 | Fix tanh layer too saturated at init |\n",
    "| Kaiming | 3.317 | 2.037 | 2.106 | Systematic method to fix tanh saturations |\n",
    "| Batch Normalization | 3.314 | 2.066 | 2.104 | Standerdize the preactivation ouptut |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6d4e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9743093",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "351b041f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('../names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aefdaa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(words)=32033\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(words)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e37cdd5",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cb02485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e648a747",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33955f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48596d87",
   "metadata": {},
   "source": [
    "<a id=\"initial-loss-tanh-saturations\"></a>\n",
    "## 1. Initial loss and Tanh saturations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dc00d6",
   "metadata": {},
   "source": [
    "### MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca7bb4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11897\n"
     ]
    }
   ],
   "source": [
    "# MLP revisited\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * 0.2\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0  \n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6a4dcf",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f91f17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000 = 3.3135\n",
      "  10000/ 200000 = 2.1648\n",
      "  20000/ 200000 = 2.3061\n",
      "  30000/ 200000 = 2.4541\n",
      "  40000/ 200000 = 1.9787\n",
      "  50000/ 200000 = 2.2930\n",
      "  60000/ 200000 = 2.4232\n",
      "  70000/ 200000 = 2.0680\n",
      "  80000/ 200000 = 2.3095\n",
      "  90000/ 200000 = 2.1207\n",
      " 100000/ 200000 = 1.8269\n",
      " 110000/ 200000 = 2.2045\n",
      " 120000/ 200000 = 1.9797\n",
      " 130000/ 200000 = 2.3946\n",
      " 140000/ 200000 = 2.1000\n",
      " 150000/ 200000 = 2.1948\n",
      " 160000/ 200000 = 1.8619\n",
      " 170000/ 200000 = 1.7809\n",
      " 180000/ 200000 = 1.9673\n",
      " 190000/ 200000 = 1.8295\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200_000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # construct minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb]  # embed characters to vector (batch_size, block_size, n_embd)==(32, 3, 10)\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate vectors\n",
    "    hpreact = embcat @ W1 + b1 # hidden layer pre activation\n",
    "    hact = torch.tanh(hpreact) # tanh activation of hidden layer\n",
    "    logits = hact @ W2 + b2  # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters: # zero grad\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # update grads\n",
    "    lr = 0.1 if i < 100_000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    if i%10_000 == 0:\n",
    "        print(f'{i:7d}/{max_steps:7d} = {loss.item():.4f}')\n",
    "    lossi.append(loss.log().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e091b1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0355966091156006\n",
      "val 2.1026782989501953\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88e0f12",
   "metadata": {},
   "source": [
    "### Gradients and Activation\n",
    "\n",
    "The hidden layer has the `tanh` activations to introduce the non-linearity in networks. `tanh` functions varies between **-1** to **+1**. Any values greater or lesser than these +1 or -1 are squashed to +1 and - 1 respectively.\n",
    "\n",
    "Histogram of pre-activations of MLP. It ranges between -15 to +15 \n",
    "\n",
    "![Hidden layer pre-activation distribution](hidden_preact.png)\n",
    "\n",
    "After `tanh` activation \n",
    "\n",
    "![Distribuion of Hidden layer tanh activation](hidden_act.png)\n",
    "\n",
    "We can clearly see the -1 and +1 are dominant values after `tanh` activation. This is a huge problem during backpropagation as The neurons with -1 and +1 outputs results in a zero gradient (or the gradient vanishes). The consequence of this is there is no update to the learning the parameters (weigths and biases) during the update step. \n",
    "\n",
    "#### What can we do?\n",
    "\n",
    "**Initialize the weigths and biases of the hidden layer to low values so that preactivation computation does not lead to huge variance.**\n",
    "\n",
    "Initializing weights and biases to random values and multiplying weight by factor of **0.2** and biases by factor of **0.01** results in following distributions.\n",
    "\n",
    "Preactivation\n",
    "\n",
    "![Hidden layer pre-activation after scaling](hidden_preact_good.png)\n",
    "\n",
    "`tanh` activation\n",
    "\n",
    "![Hidden layer activated after scaling](hidden_act_good.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcb679d",
   "metadata": {},
   "source": [
    "### Remarks:\n",
    "\n",
    "1. Initializing the neural network weights to all random values results in initial loss of `27.8817`. \n",
    "    - This is too high. Ideally, we want a network to think each of 27 character is equally likely. Which means the loss should be $$nll(\\frac{1}{27}) = -log(\\frac{1}{27})= 3.2958$$\n",
    "2. Setting the output layer biases to zero and weights to very small values by multiplying them by a factor of 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237a925f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7d72eab",
   "metadata": {},
   "source": [
    "<a id=\"kaiming-init\"></a>\n",
    "## Kaiming Init\n",
    "\n",
    "From previous section we learned that initializing the weights and biases of output layer, grounds the network from being over-confident on it's prediction during the initial loop. The hidden layer initializations causes the outputs to be very far in tails of tanh which creates the problem for network to learn parameters. So we have handpicked the initialization weights as `0.2` by analysing the results of first batch. This implies everytime we train the network we need to tune the parameter initializations of the hiddne layer(s). \n",
    "\n",
    "**Kaiming Initialization** provide a standart approach to initialize the paramters to overcome these issues.\n",
    "\n",
    "[Link to paper](https://arxiv.org/pdf/1502.01852)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b732ff9",
   "metadata": {},
   "source": [
    "### Intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "788ac7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.mean()=tensor(0.0169), x.std()=tensor(0.9965)\n",
      "y.mean()=tensor(0.0010), y.std()=tensor(1.0135)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.randn(1000, 10) # 1000 samples, 10 inputs\n",
    "w = torch.randn(10, 200) / 10**0.5  # 10 inputs, 200 neurons\n",
    "y = x @ w\n",
    "print(f\"{x.mean()=}, {x.std()=}\")\n",
    "print(f\"{y.mean()=}, {y.std()=}\")\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.subplot(121)\n",
    "plt.hist(x.view(-1).tolist(), 50, density=True);\n",
    "plt.subplot(122)\n",
    "plt.hist(y.view(-1).tolist(), 50, density=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d742b709",
   "metadata": {},
   "source": [
    "With x and w initialized ot standard normal distribution (mean=0, std=1), when we multiply the of `x` and `w` we can see the **mean remains same but the standard deviation increases/expands**. The expansion of the output distribution will result in tanh saturations.\n",
    "\n",
    "Inorder to control std. of the output distribvution we need to multiply the weights intializations by a factor. Kaiming's initializations helps to mainitain the standard deviations of the output distribution. Check the pytorch [documentation](https://docs.pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_) for differnt initialization settings.\n",
    "\n",
    "In our example we intialize by multiplying the factor $\\sqrt{1/fan_{in}}$ (where $fan_{in}$ is number of inputs to layer/neuron)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87beb785",
   "metadata": {},
   "source": [
    "### MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf5ddac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11897\n"
     ]
    }
   ],
   "source": [
    "# MLP revisited\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/(n_embd*block_size)**0.5 # Kaiming_normal factor\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0  \n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a06da3f",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d571eda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000 = 3.3179\n",
      "  10000/ 200000 = 2.1910\n",
      "  20000/ 200000 = 2.3270\n",
      "  30000/ 200000 = 2.5396\n",
      "  40000/ 200000 = 1.9468\n",
      "  50000/ 200000 = 2.3331\n",
      "  60000/ 200000 = 2.3852\n",
      "  70000/ 200000 = 2.1173\n",
      "  80000/ 200000 = 2.3159\n",
      "  90000/ 200000 = 2.2010\n",
      " 100000/ 200000 = 1.8591\n",
      " 110000/ 200000 = 2.0881\n",
      " 120000/ 200000 = 1.9389\n",
      " 130000/ 200000 = 2.3913\n",
      " 140000/ 200000 = 2.0949\n",
      " 150000/ 200000 = 2.1458\n",
      " 160000/ 200000 = 1.7824\n",
      " 170000/ 200000 = 1.7249\n",
      " 180000/ 200000 = 1.9751\n",
      " 190000/ 200000 = 1.8614\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200_000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # construct minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb]  # embed characters to vector (batch_size, block_size, n_embd)==(32, 3, 10)\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate vectors\n",
    "    hpreact = embcat @ W1 + b1 # hidden layer pre activation\n",
    "    hact = torch.tanh(hpreact) # tanh activation of hidden layer\n",
    "    logits = hact @ W2 + b2  # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters: # zero grad\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # update grads\n",
    "    lr = 0.1 if i < 100_000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    if i%10_000 == 0:\n",
    "        print(f'{i:7d}/{max_steps:7d} = {loss.item():.4f}')\n",
    "    lossi.append(loss.log().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f431e86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0376641750335693\n",
      "val 2.106989622116089\n"
     ]
    }
   ],
   "source": [
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2c380c",
   "metadata": {},
   "source": [
    "## Batch Normalizations\n",
    "\n",
    "In 2025 initialzing the parameters precisely is not important anymore due to the modern innovations. Batch Normalizations is one such innovation that helps mitigate the tanh saturations.\n",
    "\n",
    "Batch Normalization is a simple and straight forward idea developed by team in google. Standardize (subtract mean and divide by std.dev) the outputs over the mini-batch. \n",
    "\n",
    "![Batch norm](batch_norms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22b8572",
   "metadata": {},
   "source": [
    "### MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd678f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12097\n"
     ]
    }
   ],
   "source": [
    "# MLP revisited\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/(n_embd*block_size)**0.5 # Kaiming_normal factor\n",
    "# b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0  \n",
    "\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias] #[C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5844e06c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f974f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000 = 3.3147\n",
      "  10000/ 200000 = 2.1984\n",
      "  20000/ 200000 = 2.3375\n",
      "  30000/ 200000 = 2.4359\n",
      "  40000/ 200000 = 2.0119\n",
      "  50000/ 200000 = 2.2595\n",
      "  60000/ 200000 = 2.4775\n",
      "  70000/ 200000 = 2.1020\n",
      "  80000/ 200000 = 2.2788\n",
      "  90000/ 200000 = 2.1862\n",
      " 100000/ 200000 = 1.9474\n",
      " 110000/ 200000 = 2.3010\n",
      " 120000/ 200000 = 1.9837\n",
      " 130000/ 200000 = 2.4523\n",
      " 140000/ 200000 = 2.3839\n",
      " 150000/ 200000 = 2.1987\n",
      " 160000/ 200000 = 1.9733\n",
      " 170000/ 200000 = 1.8668\n",
      " 180000/ 200000 = 1.9973\n",
      " 190000/ 200000 = 1.8347\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200_000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # construct minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb]  # embed characters to vector (batch_size, block_size, n_embd)==(32, 3, 10)\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate vectors\n",
    "    # Linear layer\n",
    "    hpreact = embcat @ W1 + b1 # hidden layer pre activation\n",
    "    # Batch norm layer\n",
    "    # ---------------------------------------------------------------\n",
    "    bnmeani = hpreact.mean(0, keepdim=True)\n",
    "    bnstdi = hpreact.std(0, keepdim=True)\n",
    "    hpreact = bngain * (hpreact - bnmeani)/bnstdi + bnbias\n",
    "\n",
    "    with torch.no_grad():\n",
    "        bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "        bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "    # ---------------------------------------------------------------\n",
    "    # Tanh non-linearity\n",
    "    hact = torch.tanh(hpreact) # tanh activation of hidden layer\n",
    "    logits = hact @ W2 + b2  # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters: # zero grad\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update grads\n",
    "    lr = 0.1 if i < 100_000 else 0.01 # step learning rate decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    if i%10_000 == 0:\n",
    "        print(f'{i:7d}/{max_steps:7d} = {loss.item():.4f}')\n",
    "    lossi.append(loss.log().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65eacccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate batch norm at end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "    # pass the training set through\n",
    "    emb = C[Xtr]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    # measure mean/std over the entire training set\n",
    "    bnmean = hpreact.mean(0, keepdim=True)\n",
    "    bnstd = hpreact.std(0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e7d2b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3355,  0.6776, -0.9133,  1.0163,  1.0865,  1.0938,  1.7437, -2.1208,\n",
       "          0.5730,  1.4455, -1.6343, -2.7372, -0.4752, -0.1412, -0.0745, -1.1722,\n",
       "          0.6851, -2.6219, -0.1065,  1.6326, -0.7706, -0.3063,  0.0479,  0.6115,\n",
       "          1.1173,  0.2427,  2.0500,  0.5831,  0.8527,  1.7680, -0.3625, -0.8355,\n",
       "         -0.0854, -0.5177, -0.3806, -1.0699, -0.0786,  0.3487, -0.5808,  0.9875,\n",
       "         -0.4427, -1.3082, -0.2871, -0.2332,  0.6850,  0.6850,  2.0857, -0.7608,\n",
       "          2.3866,  1.8734,  0.8259,  0.2803,  1.8897,  0.4709,  0.6739, -1.8940,\n",
       "         -0.0401,  0.4338,  1.3760, -0.8910, -0.4524,  1.1754,  0.5613,  0.6051,\n",
       "          1.5859,  1.2261, -1.0111,  2.1495, -0.6392,  0.0938, -0.2864, -0.4856,\n",
       "          0.9632, -1.0461, -2.9990,  0.6391,  1.4327, -0.1590,  0.0941,  0.5253,\n",
       "          0.2508,  1.2521,  2.0388,  0.6608,  0.0691, -0.0813, -1.6724,  0.2933,\n",
       "          2.2423, -0.0210, -0.6666,  1.4253, -0.8412, -1.2248, -1.0129,  0.2230,\n",
       "          0.2112, -0.3226,  0.1141, -0.6350,  0.1848,  0.1165, -1.3947,  0.2343,\n",
       "          0.2170, -0.3493, -0.3409, -0.1918,  0.9244, -0.8073,  0.7147,  0.2383,\n",
       "          0.3964,  1.2354,  2.8623,  2.0470,  0.8419,  0.8480,  0.3483, -0.3153,\n",
       "         -1.0647, -1.4465,  0.3518,  1.1174, -1.1265,  0.0688, -0.2204, -0.4964,\n",
       "         -0.8558, -1.0849,  2.8098, -1.2866,  0.6543,  1.8674,  0.9511,  1.0606,\n",
       "         -0.7020,  1.8505,  0.1528,  0.4583,  1.6235, -0.1899,  1.9335,  0.2938,\n",
       "          0.4391,  0.2065,  1.1369, -0.6884,  0.1559,  0.5905, -1.5956, -0.2324,\n",
       "          1.7724,  1.2664,  0.9916, -0.4957,  1.4023, -1.6664,  0.0178,  0.4224,\n",
       "          0.5857, -0.4225,  0.9377, -1.1793, -0.4486, -0.7184, -0.3307, -0.4207,\n",
       "         -2.1519, -0.1099,  1.1733, -2.1530,  0.1215, -0.8929, -0.9959,  0.9028,\n",
       "          1.3427, -0.7526,  1.3065, -1.5016,  1.8439,  0.9704,  0.8244, -0.7247,\n",
       "          0.3621,  0.0769, -0.9657,  0.1345,  1.8784,  0.9191, -0.5020,  1.6438,\n",
       "         -0.7394,  0.2081,  0.1425,  1.2711,  1.8083, -0.8162,  0.2586,  1.2403]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "902cf58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3539,  0.6872, -0.9000,  1.0159,  1.0894,  1.0862,  1.7389, -2.1356,\n",
       "          0.5608,  1.4246, -1.6445, -2.7426, -0.4861, -0.1510, -0.0687, -1.1550,\n",
       "          0.6891, -2.6399, -0.1283,  1.6240, -0.7732, -0.2864,  0.0467,  0.6119,\n",
       "          1.1172,  0.2433,  2.0542,  0.5778,  0.8515,  1.7729, -0.3741, -0.8385,\n",
       "         -0.0831, -0.5198, -0.3817, -1.0699, -0.0781,  0.3370, -0.5769,  0.9935,\n",
       "         -0.4507, -1.3313, -0.2895, -0.2298,  0.6877,  0.6936,  2.0835, -0.7759,\n",
       "          2.3804,  1.8614,  0.8118,  0.2735,  1.8802,  0.4705,  0.6656, -1.8962,\n",
       "         -0.0420,  0.4356,  1.3924, -0.8906, -0.4676,  1.1688,  0.5539,  0.6001,\n",
       "          1.5853,  1.2103, -1.0171,  2.1422, -0.6330,  0.1071, -0.2926, -0.4831,\n",
       "          0.9506, -1.0144, -2.9925,  0.6269,  1.4404, -0.1574,  0.0955,  0.5159,\n",
       "          0.2487,  1.2401,  2.0104,  0.6695,  0.0768, -0.0851, -1.6768,  0.2963,\n",
       "          2.2374, -0.0100, -0.6669,  1.4356, -0.8431, -1.2317, -1.0220,  0.2201,\n",
       "          0.1928, -0.3261,  0.1108, -0.6206,  0.1795,  0.1089, -1.4007,  0.2215,\n",
       "          0.2301, -0.3369, -0.3340, -0.1849,  0.9342, -0.8288,  0.7119,  0.2475,\n",
       "          0.3813,  1.2447,  2.8427,  2.0338,  0.8295,  0.8458,  0.3483, -0.3078,\n",
       "         -1.0773, -1.4394,  0.3424,  1.1274, -1.1252,  0.0692, -0.2308, -0.4936,\n",
       "         -0.8471, -1.0785,  2.8080, -1.2823,  0.6541,  1.8701,  0.9662,  1.0696,\n",
       "         -0.7051,  1.8326,  0.1624,  0.4782,  1.6376, -0.1962,  1.9364,  0.2986,\n",
       "          0.4300,  0.2062,  1.1390, -0.6907,  0.1571,  0.5883, -1.5947, -0.2230,\n",
       "          1.7796,  1.2689,  0.9813, -0.4950,  1.4052, -1.6645,  0.0249,  0.4152,\n",
       "          0.5742, -0.4044,  0.9335, -1.1684, -0.4574, -0.7229, -0.3192, -0.4214,\n",
       "         -2.1397, -0.1036,  1.1620, -2.1602,  0.1196, -0.8846, -0.9970,  0.9115,\n",
       "          1.3361, -0.7411,  1.2991, -1.5061,  1.8279,  0.9732,  0.8192, -0.7259,\n",
       "          0.3556,  0.0714, -0.9718,  0.1299,  1.8912,  0.9107, -0.4950,  1.6493,\n",
       "         -0.7601,  0.2008,  0.1606,  1.2586,  1.8110, -0.8021,  0.2453,  1.2369]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnmean_running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0359aefd",
   "metadata": {},
   "source": [
    "`bnmean` and `bnmean_running` should be approximately the same. This is similar intuition behined the std dev too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0a83c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.06659197807312\n",
      "val 2.1050572395324707\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  # hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True))/hpreact.std(0, keepdim=True) + bnbias\n",
    "  hpreact = bngain * (hpreact - bnmean_running)/bnstd_running + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc16a36d",
   "metadata": {},
   "source": [
    "#### Second order effect of Batch normalization\n",
    "\n",
    "Prior to Batch normalization, one input sample only impacts its logits at the output. Since we are batch normalizing the preactivation outputs based on all samples in mini-batch (we are mathematically tying other samples in mini-batch to this sample). The other samples that have come in for the ride has impact on the output of batch norm and so the output logits. This causes the outputs to be **jittery**.\n",
    "\n",
    "This process of normalizing over mini-batch, indirectly causes a **regularizing effect** making the neural net not to overfit the output on the given input. This effect makes the approach even more powerful for training the neural networks.\n",
    "\n",
    "\n",
    "#### Notes\n",
    "1. We dont have to seperately compute the `bnmean` and `bnstd` after training. Instead we can keep track of the running mean and std. values as given in the [paper](https://arxiv.org/pdf/1502.03167)\n",
    "2. `bias` term of the preactivation layer of hidden unit cancels itself out in the batch normalization step. The bias term in this step has no effect on the network. Moreover batch normalization has its own bias so there is no need for the preactivation layer bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e70231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
