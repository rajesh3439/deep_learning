{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e059fd2",
   "metadata": {},
   "source": [
    "# Exercise 02 \n",
    "\n",
    "BatchNorm, unlike other normalization layers like LayerNorm/GroupNorm etc. has the big advantage that after training, the batchnorm gamma/beta can be \"folded into\" the weights of the preceeding Linear layers, effectively erasing the need to forward it at test time. Set up a small 3-layer MLP with batchnorms, train the network, then \"fold\" the batchnorm gamma/beta into the preceeding Linear layer's W,b by creating a new W2, b2 and erasing the batch norm. Verify that this gives the same forward pass during inference. i.e. we see that the batchnorm is there just for stabilizing the training, and can be thrown out after training is done! pretty cool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aecea074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf107cc",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c2ba81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('../names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6419d5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(words)=32033\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(words)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d17674",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3b359ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d9e7ef",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "912adea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c4a999",
   "metadata": {},
   "source": [
    "## Pytorch Layers: Linear & Batch-norm implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dd3ea8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, fan_in:int, fan_out:int, _generator:torch.Generator, bias:bool = True) -> None:\n",
    "        self.W = torch.randn((fan_in,fan_out), generator=_generator) / fan_in**0.5\n",
    "        self.b = torch.randn(fan_out, generator=_generator) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.W\n",
    "        if self.b is not None:\n",
    "            self.out += self.b\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.W] + ([self.b] if self.b is not None else [])\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1) -> None:\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        # parameters (trained with back-prop)\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        # buffers (trained with running-momentum update)\n",
    "        self._running_mean = torch.zeros(dim)\n",
    "        self._running_var = torch.ones(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # forward-pass\n",
    "        if self.training:\n",
    "            xmean = x.mean(0, keepdim=True) # batch mean\n",
    "            xvar = x.var(0, keepdim=True, unbiased=True) # batch var\n",
    "        else:\n",
    "            xmean = self._running_mean\n",
    "            xvar = self._running_var\n",
    "        xhat = (x - xmean)/torch.sqrt(xvar + self.eps) # normalization  of batch\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "\n",
    "        # update buffers\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self._running_mean = (1-self.momentum) * self._running_mean + self.momentum * xmean\n",
    "                self._running_var = (1-self.momentum) * self._running_var + self.momentum * xvar\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "class FusedLinear:\n",
    "    def __init__(self, lin:Linear, bn:BatchNorm1d)->None:\n",
    "        self.lin = lin\n",
    "        self.bn = bn\n",
    "        self.W, self.b = self._compute_w_b()\n",
    "\n",
    "    def _compute_w_b(self):\n",
    "        # folding gammma and beta into previous layers W & b\n",
    "        W, b = self.lin.W, self.lin.b\n",
    "        gamma, beta = self.bn.gamma, self.bn.beta\n",
    "        mu, sigma = self.bn._running_mean, self.bn._running_var**0.5\n",
    "        gs = gamma/sigma\n",
    "        W2 = gs*W\n",
    "        b2 = gs*b + beta - mu*gs\n",
    "        return W2, b2\n",
    "\n",
    "    def __call__(self,x):\n",
    "        self.out = x @ self.W + self.b\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self,x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a1cf5b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16651\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 27\n",
    "n_embed = 10 # dimensions of embedding\n",
    "n_hidden = 100 # number of neurons in hidden layer\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "C = torch.randn((vocab_size, n_embed), generator=g)\n",
    "layers = [\n",
    "    Linear(n_embed*block_size,  n_hidden, g), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden,            n_hidden, g), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden,          vocab_size, g), BatchNorm1d(vocab_size),\n",
    "]\n",
    "\n",
    "# initializations\n",
    "with torch.no_grad():\n",
    "    # make last layer weights and bias to 0\n",
    "    layers[-1].gamma *= 0.1  \n",
    "    # apply kaiming init to other layers\n",
    "    for layer in layers[:-1]:\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.W *= 1\n",
    "\n",
    "parameters = [C] + [p  for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters)) # total number of parameters\n",
    "\n",
    "# set requries grad flag\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f460e39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000:3.309078\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200_000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "ud = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # mini-batch\n",
    "    ix = torch.randint(0,Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb,Yb = Xtr[ix], Ytr[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb]\n",
    "    x = emb.view(emb.shape[0], -1)\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x, Yb)\n",
    "\n",
    "    # backward pass\n",
    "    for layer in layers:\n",
    "        layer.out.retain_grad()\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100_000 else 0.01 # learning rate with decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    if i%10_000 == 0:\n",
    "        print(f\"{i:7d}/{max_steps:7d}:{loss.item():4f}\")\n",
    "    lossi.append(loss.log10().item())\n",
    "    with torch.no_grad():\n",
    "        ud.append([(lr*p.grad.std()/(p.data.std() + 1e-5)).log10().item()  for p in parameters])\n",
    "\n",
    "    if i == 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ad5d77",
   "metadata": {},
   "source": [
    "### Mathetatical unfolding of linear and batch norm layers\n",
    "The output of linear layer\n",
    "$$x_l = W.x + b$$\n",
    "\n",
    "This feeds into the batch-norm layer. The output is \n",
    "$$x_b = \\gamma . \\hat{x_l} + \\beta$$\n",
    "where, \n",
    "$$ \\hat{x_l} = \\frac{x_l - \\mu}{\\sigma}$$\n",
    "\n",
    "Expanding the formula and reordering it\n",
    "$$x_b = \\frac{\\gamma}{\\sigma} . x_l + \\left( \\beta - \\frac{\\mu \\gamma}{\\sigma} \\right)$$\n",
    "\n",
    "Substituting $x_l$ to above equation \n",
    "$$x_b = \\frac{\\gamma}{\\sigma} (W.x + b) + \\left( \\beta - \\frac{\\mu \\gamma}{\\sigma} \\right)$$\n",
    "\n",
    "rearranging to $Wx+b$ format\n",
    "$$x_b = \\frac{\\gamma}{\\sigma}.W. x + \\left( \\frac{\\gamma}{\\sigma}. b + \\left( \\beta - \\frac{\\mu \\gamma}{\\sigma} \\right) \\right)$$\n",
    "\n",
    "So, finally\n",
    "$$W2 = \\frac{\\gamma}{\\sigma}.W ; b2 = \\left( \\frac{\\gamma}{\\sigma}. b + \\left( \\beta - \\frac{\\mu \\gamma}{\\sigma} \\right) \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cfa68cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set training of batchnorm layer to false\n",
    "for layer in layers:\n",
    "    if isinstance(layer, BatchNorm1d):\n",
    "        layer.training = False\n",
    "\n",
    "with torch.no_grad():\n",
    "    emb = C[Xtr[:3]]\n",
    "    x = emb.view(emb.shape[0], -1)\n",
    "\n",
    "    # noraml forward pass on first linear-batch layers\n",
    "    x0 = layers[0](x)   # First linear layer\n",
    "\n",
    "    x1 = layers[1](x0)  # Batch norm layer\n",
    "\n",
    "    # folding gammma and beta into previous layers W & b\n",
    "    W, b = layers[0].W, layers[0].b\n",
    "    gamma, beta = layers[1].gamma, layers[1].beta\n",
    "    mu, sigma = layers[1]._running_mean, layers[1]._running_var**0.5\n",
    "    gs = gamma/sigma\n",
    "    W2 = gs*W\n",
    "    b2 = gs*b + beta - mu*gs\n",
    "    x1_uf = x@W2 + b2 \n",
    "\n",
    "\n",
    "    # print(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ba62759a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100]) torch.Size([1, 100]) torch.Size([100]) torch.Size([100]) torch.Size([1, 100])\n",
      "torch.Size([3, 30]) torch.Size([30, 100])\n"
     ]
    }
   ],
   "source": [
    "print(mu.shape, sigma.shape, gamma.shape, beta.shape, gs.shape)\n",
    "print(x.shape, W2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "573a2b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1793,  0.2994, -0.4260,  0.5528, -0.0344,  0.5147, -0.3274, -0.0393,\n",
       "        -0.2055,  0.3896, -0.2481, -0.7650, -0.0507, -0.4181, -0.9867,  0.3624,\n",
       "         0.6038, -1.0864, -0.7159, -1.7000, -0.3483,  0.6470, -0.4465, -0.2338,\n",
       "         0.0991,  0.0341, -0.8191, -0.8012, -0.9569, -0.3122, -1.5597,  0.9804,\n",
       "         0.7465, -1.3748,  0.5566,  0.1926, -0.2842,  0.7759, -0.2604, -0.4401,\n",
       "        -1.4257,  1.6296, -0.9216,  1.5503,  0.8523,  0.8311,  1.8679,  0.1112,\n",
       "        -0.3980,  0.5268, -0.4364, -1.4508, -1.0315, -0.0180,  0.4832,  0.7953,\n",
       "         0.4846,  0.2712, -0.5988, -0.3970,  0.8105, -0.1421,  0.1642,  2.1538,\n",
       "         0.5010, -0.7763,  0.9983,  0.0344,  0.9966,  0.8128, -0.3665, -0.2647,\n",
       "         0.4427,  0.7612,  0.1248,  0.3893, -0.0479,  0.8442,  0.3901,  0.5991,\n",
       "         0.7502, -0.3544, -1.7815, -0.6673, -1.1491,  0.3127, -0.4502, -1.4068,\n",
       "        -0.2253,  1.2449, -1.5305, -0.1753,  0.2042, -0.3764,  2.3007,  0.6251,\n",
       "        -1.1372, -1.2816, -0.7786, -0.8442])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1_uf[-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bd68373c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1793,  0.2994, -0.4260,  0.5528, -0.0344,  0.5147, -0.3274, -0.0393,\n",
       "        -0.2055,  0.3896, -0.2481, -0.7650, -0.0507, -0.4181, -0.9867,  0.3624,\n",
       "         0.6038, -1.0863, -0.7159, -1.7000, -0.3483,  0.6470, -0.4465, -0.2338,\n",
       "         0.0991,  0.0341, -0.8191, -0.8012, -0.9569, -0.3122, -1.5597,  0.9804,\n",
       "         0.7465, -1.3748,  0.5566,  0.1926, -0.2842,  0.7759, -0.2604, -0.4401,\n",
       "        -1.4256,  1.6296, -0.9216,  1.5503,  0.8523,  0.8311,  1.8679,  0.1112,\n",
       "        -0.3980,  0.5267, -0.4364, -1.4508, -1.0315, -0.0180,  0.4832,  0.7953,\n",
       "         0.4846,  0.2712, -0.5988, -0.3970,  0.8105, -0.1421,  0.1642,  2.1537,\n",
       "         0.5010, -0.7763,  0.9983,  0.0344,  0.9966,  0.8128, -0.3665, -0.2647,\n",
       "         0.4427,  0.7612,  0.1248,  0.3893, -0.0479,  0.8442,  0.3901,  0.5991,\n",
       "         0.7502, -0.3544, -1.7814, -0.6673, -1.1491,  0.3126, -0.4502, -1.4068,\n",
       "        -0.2253,  1.2449, -1.5305, -0.1753,  0.2042, -0.3764,  2.3007,  0.6251,\n",
       "        -1.1372, -1.2816, -0.7786, -0.8442])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231c78ae",
   "metadata": {},
   "source": [
    "Folding the batchnorm layer into linear layer we obtain identical results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6a52f920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FusedLinear', 'Tanh', 'FusedLinear', 'Tanh', 'FusedLinear']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new layers\n",
    "comb_layers = []\n",
    "for i, layer in enumerate(layers):\n",
    "    if isinstance(layer, BatchNorm1d):\n",
    "        comb_layers.append(FusedLinear(layers[i-1], layer))\n",
    "    elif isinstance(layer, Tanh):\n",
    "        comb_layers.append(layer)\n",
    "\n",
    "\n",
    "\n",
    "[layer.__class__.__name__ for layer in comb_layers]\n",
    "\n",
    "# len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2931a28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set training of batchnorm layer to false\n",
    "for layer in layers:\n",
    "    if isinstance(layer, BatchNorm1d):\n",
    "        layer.training = False\n",
    "\n",
    "@torch.no_grad()\n",
    "def split_loss(split, layers):\n",
    "    X,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "    }[split]\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[X] # (N, block_size, n_embd)\n",
    "    x = emb.view(emb.shape[0], -1)\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x, y)\n",
    "\n",
    "    print(split, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "58062c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val 2.4326910972595215\n",
      "test 2.4375462532043457\n"
     ]
    }
   ],
   "source": [
    "split_loss('val', layers)\n",
    "split_loss('test', layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a50d7b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val 2.432690143585205\n",
      "test 2.4375455379486084\n"
     ]
    }
   ],
   "source": [
    "split_loss('val', comb_layers)\n",
    "split_loss('test', comb_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072679d6",
   "metadata": {},
   "source": [
    "The losses obtained after combining the layers are very similar to loss obtained with original model sandwidch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94e9f86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
